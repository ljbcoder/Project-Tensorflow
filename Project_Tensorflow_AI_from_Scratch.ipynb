{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXTvlfL9pDsJb3OgH2TCPh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljbcoder/Project-Tensorflow/blob/main/Project_Tensorflow_AI_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW3bewxyeALA"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "#Make sure that softmax function can be used without\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "def softmax_derivative(z):\n",
        "    s = softmax(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "\n",
        "def MSE(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "class ModelLayer:\n",
        "    def __init__(self, output_size, activation='relu', input_size=None, learning_rate=0.1):\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size  # Set later if None\n",
        "        self.activation_name = activation.lower()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.activation = None\n",
        "        self.activation_derivative = None\n",
        "        self.weights = None  # Will be initialized when input_size is known\n",
        "        self.bias = None\n",
        "\n",
        "        self.input_vals = None\n",
        "        self.z = None\n",
        "\n",
        "    def build(self, input_size):\n",
        "        \"\"\"Initialize weights and biases once input size is known\"\"\"\n",
        "        self.input_size = input_size\n",
        "\n",
        "        if self.activation_name == 'relu':\n",
        "            # He initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = relu\n",
        "            self.activation_derivative = relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'leaky_relu':\n",
        "            # He initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = leaky_relu\n",
        "            self.activation_derivative = leaky_relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'sigmoid':\n",
        "            # Xavier initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(1. / input_size)\n",
        "            self.activation = sigmoid\n",
        "            self.activation_derivative = sigmoid_derivative\n",
        "\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {self.activation_name}\")\n",
        "\n",
        "        self.bias = np.zeros((self.output_size, 1))\n",
        "\n",
        "\n",
        "    def pass_forward(self, input_vals):\n",
        "        if self.weights is None:\n",
        "            self.build(input_vals.shape[0])\n",
        "        self.input_vals = input_vals\n",
        "        self.z = np.dot(self.weights, input_vals) + self.bias\n",
        "        return self.activation(self.z)\n",
        "\n",
        "    def pass_backwards(self, dA):\n",
        "        dZ = dA * self.activation_derivative(self.z)\n",
        "        dW = np.dot(dZ, self.input_vals.T)\n",
        "        dB = dZ\n",
        "        dA_prev = np.dot(self.weights.T, dZ)\n",
        "\n",
        "        self.weights -= self.learning_rate * dW\n",
        "        self.bias -= self.learning_rate * dB\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.built = False\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.built:\n",
        "            self._build_layers(x.shape[0])\n",
        "            self.built = True\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.pass_forward(x)\n",
        "        return x\n",
        "\n",
        "    def _build_layers(self, input_size):\n",
        "        for layer in self.layers:\n",
        "            if layer.input_size is None:\n",
        "                layer.build(input_size)\n",
        "            input_size = layer.output_size  # next layer's input\n",
        "\n",
        "    def backward(self, dLoss):\n",
        "        for layer in reversed(self.layers):\n",
        "            dLoss = layer.pass_backwards(dLoss)\n",
        "\n",
        "    def train(self, x, y_true, epochs=1000, print_every=100):\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(x)\n",
        "            loss = np.mean((y_pred - y_true) ** 2)\n",
        "            dLoss = 2 * (y_pred - y_true)\n",
        "            self.backward(dLoss)\n",
        "\n",
        "            if epoch % print_every == 0 or epoch == epochs - 1:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, x):\n",
        "      return self.forward(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "  s = sigmoid(z)\n",
        "  return s*(1-s)\n",
        "\n",
        "def relu(z):\n",
        "  return np.max(0,z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "  return (z>0).astype(float)\n",
        "\n",
        "def leaky_relu(z,alpha=0.01):\n",
        "  if z>0:\n",
        "    return z\n",
        "  else:\n",
        "    return alpha*z\n",
        "\n",
        "def leaky_relu_derivative(x,alpha=0.01):\n",
        "  return np.where(x>0,1,alpha)"
      ],
      "metadata": {
        "id": "N9_Ve-ObK70t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model = Model()\n",
        "model.add(ModelLayer(4, activation='leaky_relu'))\n",
        "\n",
        "\n",
        "x = np.random.rand(4, 1)\n",
        "y = np.array([[1], [1], [0], [0]])\n",
        "\n",
        "model.train(x, y, epochs=100)\n",
        "model.predict(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmf-_M9aeVLp",
        "outputId": "85135577-6492-4e4d-ce55-b70658c34885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3757\n",
            "Epoch 99, Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.00000000e+00],\n",
              "       [ 1.00000000e+00],\n",
              "       [ 5.55111512e-17],\n",
              "       [-2.33480947e-03]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate 100 house sizes between 500 and 3500 sqft\n",
        "sqft = np.random.uniform(500, 3500, size=(1, 100))  # shape (1, 100)\n",
        "price = 200 * sqft + 50000                          # price = 200 * sqft + 50k\n",
        "price += np.random.randn(1, 100) * 10000            # add some noises\n"
      ],
      "metadata": {
        "id": "OPkLoNhdp6l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sqft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knccx6_pqtC4",
        "outputId": "824f9736-5603-4924-af9f-f67b511a245f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2980.49861598 1237.73686347 1834.56578602 3382.66469291 3080.4852769\n",
            "  1183.4930693  3395.29683719 1879.80756321 2822.49843546 1047.35946541\n",
            "  1308.15349093 1431.65884865 1877.17367375 2907.42946542 3082.95481563\n",
            "  2748.92033657 1437.35365892 2280.07795001 1816.17470386  681.88729838\n",
            "  1055.34371559 2212.07004786 1890.03155819  862.72219819 1626.74832708\n",
            "  1652.47289241 3392.52950772 3391.98972574 2289.90388507 2274.51640772\n",
            "  2265.47451938  834.71503874 1766.41082394 3438.26816673  688.97024033\n",
            "  3088.38256311 1915.1392962  2668.27822782 2541.99563777 2324.62082712\n",
            "  3017.94449106  630.56874837 3378.4010847   953.65738227 3050.51406112\n",
            "  1037.50239164  609.08768025 2701.88319766 2112.51094664  767.37928316\n",
            "  2329.17319737 1349.10851325  829.58782146 1983.12104622 3014.62872154\n",
            "  1520.16643913  763.64246404 1702.98646568 2296.41483737 1786.05642725\n",
            "  1353.71245486 2210.9949241  1834.77696569 1837.31699784 1624.05955706\n",
            "  3208.19480668 1042.92163426  659.53200576 2527.6700721  1495.58044971\n",
            "  1052.76536559 3241.57272954 3195.44135208 3497.3604345  2635.50199674\n",
            "   548.35550244 2394.88835842 2709.41321762  613.8119166  2883.38072475\n",
            "   703.37400131 2431.09079162 3011.76084366 2608.5764668  1333.09373945\n",
            "  1635.22380659 1857.28361208 2896.47668544 1476.4880311  1684.78905098\n",
            "  1687.86701326 2835.44973024 2041.23459694 3164.53192715 1196.45354575\n",
            "   962.52493933 2685.33196005 1080.90391571 3361.1498039  2967.77454932]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housemodel = Model()\n",
        "housemodel.add(ModelLayer(1, activation='linear', input_size=1, learning_rate=1e-7))\n",
        "\n",
        "  # Use low learning rate due to large outputs\n",
        "housemodel.train(sqft, price, epochs=1000, print_every=100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "6O8QYh8Rqjik",
        "outputId": "e93b343b-5c1c-4ff0-a0df-87747cfc07f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-843afa3e5115>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Use low learning rate due to large outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhousemodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-6a8f28f6e9fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y_true, epochs, print_every)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-6a8f28f6e9fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-6a8f28f6e9fb>\u001b[0m in \u001b[0;36mpass_forward\u001b[0;34m(self, input_vals)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_vals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layers[0].weights)\n",
        "print(model.layers[0].bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO9fA4wlesGJ",
        "outputId": "61077234-aeb9-44d4-95eb-d876581c0479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.78063498 -0.46442502 -0.41676732 -0.03846604]\n",
            " [-0.09199558  0.88326973  0.52355762 -1.23248312]\n",
            " [ 0.68986584 -0.55300088  0.02077039 -0.06488225]\n",
            " [-1.04242472  0.96643341 -0.01569079 -0.53808634]]\n",
            "[[ 0.        ]\n",
            " [-0.08229945]\n",
            " [-0.09500813]\n",
            " [ 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BufferedIOBase\n",
        "import numpy as np\n",
        "\n",
        "I = np.array([[0.05,.10]])\n",
        "T = np.array([[0,1]])\n",
        "WH = np.array([[0.15, 0.25],\n",
        "               [0.2,0.3]])\n",
        "\n",
        "BH = np.array([[0.35,0.35]])\n",
        "WO = np.array([[0.4, 0.5],\n",
        "               [0.45,0.55]])\n",
        "BO = np.array([[0.6,.6]])\n",
        "\n",
        "for epoch in range(100000):\n",
        "  print(epoch)\n",
        "\n",
        "  #Apply sigmoid to first layer\n",
        "  H = I @ WH + BH\n",
        "  H = 1/(1+np.exp(-H))\n",
        "\n",
        "  #Apply Softmax for second layer\n",
        "  O = H @ WO + BO\n",
        "  OM = O - np.max(O)\n",
        "  O = np.exp(OM)/np.sum(np.exp(OM))\n",
        "\n",
        "  E = np.sum(-T*np.log(O))\n",
        "  if E < 0.0001:\n",
        "    break\n",
        "\n",
        "  Ob = O - T\n",
        "  #Nothing for softmax cross entropy error\n",
        "\n",
        "  Hb = Ob @ WO.T\n",
        "  Hb = Hb * H * (1-H)\n",
        "\n",
        "  WHb = I.T @ Hb\n",
        "  BHb = 1* Hb\n",
        "  WOb = H.T @ Ob\n",
        "  BOb = 1 * Ob\n",
        "\n",
        "  lr = 0.01\n",
        "  WH = WH - lr * WHb\n",
        "  BH = BH - lr * BHb\n",
        "  WO = WO - lr * WOb\n",
        "  BO = BO - lr - BOb\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mi5kWcIWqHHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL CODE"
      ],
      "metadata": {
        "id": "8f-QwhcYCjQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "ww4fmNlykgMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train.reshape(-1, 28*28).T / 255.0  # shape: (784, num_samples)\n",
        "x_test = x_test.reshape(-1, 28*28).T / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_encoded = to_categorical(y_train).T  # shape: (10, num_samples)\n",
        "y_test_encoded = to_categorical(y_test).T\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9ctQJqIkoFa",
        "outputId": "98cd12ee-ce35-416b-a724-7f859b6af975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_encoded[:,1:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpO3hb7tJcz_",
        "outputId": "efadec86-214a-4a30-cf6f-91f0f7ef54d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "#Softmax derivative is implicit, and handled by cross entropy\n",
        "# def softmax_derivative(z):\n",
        "#     s = softmax(z)\n",
        "#     return s * (1 - s)\n",
        "\n",
        "#Loss Functions\n",
        "def MSE(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def MSE_derivative(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true)\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    epsilon = 1e-15  # Small value to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip values to avoid log(0)\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "\n",
        "def cross_entropy_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true\n",
        "\n",
        "class ModelLayer:\n",
        "    def __init__(self, output_size, activation='relu', input_size=None, learning_rate=0.001):\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size  # Set later if None\n",
        "        self.activation_name = activation.lower()\n",
        "        self.activation = None\n",
        "        self.activation_derivative = None\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights = None  # Will be initialized when input_size is known\n",
        "        self.bias = None\n",
        "\n",
        "        self.input_vals = None\n",
        "        self.z = None\n",
        "\n",
        "    def build(self, input_size):\n",
        "        \"\"\"Initialize weights and biases once input size is known\"\"\"\n",
        "        self.input_size = input_size\n",
        "\n",
        "        if self.activation_name == 'relu':\n",
        "            # He initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = relu\n",
        "            self.activation_derivative = relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'leaky_relu':\n",
        "            # He initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = leaky_relu\n",
        "            self.activation_derivative = leaky_relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'sigmoid':\n",
        "            # Xavier initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(1. / input_size)\n",
        "            self.activation = sigmoid\n",
        "            self.activation_derivative = sigmoid_derivative\n",
        "\n",
        "        elif self.activation_name == 'softmax':\n",
        "            # Xavier initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(1. / input_size)\n",
        "            self.activation = softmax\n",
        "            self.activation_derivative = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {self.activation_name}\")\n",
        "\n",
        "        self.bias = np.zeros((self.output_size, 1))\n",
        "\n",
        "\n",
        "    def pass_forward(self, input_vals):\n",
        "        if self.weights is None:\n",
        "            self.build(input_vals.shape[0])\n",
        "        self.input_vals = input_vals\n",
        "        self.z = np.dot(self.weights, input_vals) + self.bias\n",
        "        return self.activation(self.z)\n",
        "\n",
        "    def pass_backwards(self, dA):\n",
        "        if self.activation_name == 'softmax':\n",
        "            dZ = dA  # Softmax derivative handled implicitly --> cross-entropy\n",
        "        else:\n",
        "            dZ = dA * self.activation_derivative(self.z)\n",
        "\n",
        "        dW = np.dot(dZ, self.input_vals.T)\n",
        "        dB = np.sum(dZ, axis=1, keepdims=True)  # Sum across the batch size for biases (Need for dim)\n",
        "        dA_prev = np.dot(self.weights.T, dZ)\n",
        "\n",
        "        self.weights -= self.learning_rate * dW\n",
        "        self.bias -= self.learning_rate * dB\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self,loss_function,layers = []):\n",
        "        self.layers = layers\n",
        "        self.built = False\n",
        "        self.loss_function_name = loss_function.lower()\n",
        "\n",
        "        #Apply the loss function\n",
        "        if self.loss_function_name == 'mse':\n",
        "          self.loss_function = MSE\n",
        "          self.loss_function_derivative = MSE_derivative\n",
        "\n",
        "        elif self.loss_function_name == 'cross_entropy':\n",
        "          self.loss_function = cross_entropy\n",
        "          self.loss_function_derivative = cross_entropy_derivative\n",
        "\n",
        "        else:\n",
        "          raise ValueError(f\"Unsupported activation: {loss_function}\")\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.built:\n",
        "            self._build_layers(x.shape[0])\n",
        "            self.built = True\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.pass_forward(x)\n",
        "        return x\n",
        "\n",
        "    def _build_layers(self, input_size):\n",
        "        for layer in self.layers:\n",
        "            if layer.input_size is None:\n",
        "                layer.build(input_size)\n",
        "            input_size = layer.output_size  # next layer's input\n",
        "\n",
        "    def backward(self, dLoss):\n",
        "        for layer in reversed(self.layers):\n",
        "            dLoss = layer.pass_backwards(dLoss)\n",
        "\n",
        "    #Change so you can apply batch gradient descent, minibatch, and normal\n",
        "    def train(self, x, y_true, epochs=20, print_every=1):\n",
        "        for epoch in range(epochs):\n",
        "            for start in range(0, len(x[0]), 100):  # Process in batches of 100 samples\n",
        "                x_batch = x[:, start:start+100]  # Shape: (784, 100)\n",
        "                y_batch = y_true[:, start:start+100]  # Shape: (10, 100)\n",
        "\n",
        "                # Forward pass\n",
        "                y_pred = self.forward(x_batch)  # Shape: (10, 100)\n",
        "\n",
        "                # Compute the loss for the entire batch\n",
        "                loss = self.loss_function(y_batch, y_pred)  # Shape: (10, 100)\n",
        "                dLoss = self.loss_function_derivative(y_batch, y_pred)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(dLoss)\n",
        "\n",
        "            if epoch % print_every == 0 or epoch == epochs - 1:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "      return self.forward(x)\n"
      ],
      "metadata": {
        "id": "yoreYrRd6cs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(loss_function = \"cross_entropy\")\n",
        "model.add(ModelLayer(128, activation='relu'))\n",
        "model.add(ModelLayer(64, activation='relu'))\n",
        "model.add(ModelLayer(32, activation='relu'))\n",
        "model.add(ModelLayer(10, activation='softmax'))\n",
        "\n",
        "\n",
        "model.train(x_train[:,:1000], y_train_encoded[:,:1000], epochs=500, print_every=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "2ymrzU1kAOpq",
        "outputId": "63f8b47b-edb1-4373-9146-b8c113c2ac38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 206.6819\n",
            "Epoch 100, Loss: 0.3837\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-e7d494de7d53>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-82-5627370ace1e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y_true, epochs, print_every)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (10, 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;31m# Compute the loss for the entire batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-5627370ace1e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-5627370ace1e>\u001b[0m in \u001b[0;36mpass_forward\u001b[0;34m(self, input_vals)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_vals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "#Softmax derivative is implicit, and handled by cross entropy\n",
        "# def softmax_derivative(z):\n",
        "#     s = softmax(z)\n",
        "#     return s * (1 - s) --> This eventually cancels out with 1/s multiplied in the backpropagation step\n",
        "\n",
        "#Loss Functions\n",
        "def MSE(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def MSE_derivative(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true)\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    epsilon = 1e-10  # Small value to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Clip values to avoid log(0)\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "\n",
        "def cross_entropy_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true\n",
        "\n",
        "class ModelLayer:\n",
        "    def __init__(self, output_size, activation='relu', input_size=None, learning_rate=0.001):\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size  # Set later if None\n",
        "        self.activation_name = activation.lower()\n",
        "        self.activation = None\n",
        "        self.activation_derivative = None\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights = None  # Will be initialized when input_size is known\n",
        "        self.bias = None\n",
        "\n",
        "        self.input_vals = None\n",
        "        self.z = None\n",
        "\n",
        "    def build(self, input_size):\n",
        "        \"\"\"Initialize weights and biases once input size is known\"\"\"\n",
        "        self.input_size = input_size\n",
        "\n",
        "        if self.activation_name == 'relu':\n",
        "            # He initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = relu\n",
        "            self.activation_derivative = relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'leaky_relu':\n",
        "            # He initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = leaky_relu\n",
        "            self.activation_derivative = leaky_relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'sigmoid':\n",
        "            # Xavier initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(1. / input_size)\n",
        "            self.activation = sigmoid\n",
        "            self.activation_derivative = sigmoid_derivative\n",
        "\n",
        "        elif self.activation_name == 'softmax':\n",
        "            # Xavier initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(1. / input_size)\n",
        "            self.activation = softmax\n",
        "            self.activation_derivative = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {self.activation_name}\")\n",
        "\n",
        "        self.bias = np.zeros((self.output_size, 1))\n",
        "\n",
        "\n",
        "    def pass_forward(self, input_vals):\n",
        "        if self.weights is None:\n",
        "            self.build(input_vals.shape[0])\n",
        "        self.input_vals = input_vals\n",
        "        self.z = np.dot(self.weights, input_vals) + self.bias\n",
        "        return self.activation(self.z)\n",
        "\n",
        "    def pass_backwards(self, dA):\n",
        "        if self.activation_name == 'softmax':\n",
        "            dZ = dA  # Softmax derivative is handled implicitly in cross-entropy\n",
        "        else:\n",
        "          dZ = dA * self.activation_derivative(self.z)\n",
        "\n",
        "        dW = np.dot(dZ, self.input_vals.T)\n",
        "        dB = np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = np.dot(self.weights.T, dZ)\n",
        "\n",
        "        self.weights -= self.learning_rate * dW\n",
        "        self.bias -= self.learning_rate * dB\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "    def reset_weights(self):\n",
        "        self.build(self.input_size)\n",
        "\n",
        "class Model:\n",
        "    def __init__(self,layers = []):\n",
        "        self.layers = layers\n",
        "        self.built = False\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.built:\n",
        "            self._build_layers(x.shape[0])\n",
        "            self.built = True\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.pass_forward(x)\n",
        "        return x\n",
        "\n",
        "    def _build_layers(self, input_size):\n",
        "        for layer in self.layers:\n",
        "            if layer.input_size is None:\n",
        "                layer.build(input_size)\n",
        "            input_size = layer.output_size  # next layer's input\n",
        "\n",
        "    def compile(self, optimizer, loss_function):\n",
        "        self.optimizer = optimizer.lower()\n",
        "\n",
        "        self.loss_function_name = loss_function.lower()\n",
        "\n",
        "        #Apply the loss function\n",
        "        if self.loss_function_name == 'mse':\n",
        "          self.loss_function = MSE\n",
        "          self.loss_function_derivative = MSE_derivative\n",
        "\n",
        "        elif self.loss_function_name == 'cross_entropy':\n",
        "          self.loss_function = cross_entropy\n",
        "          self.loss_function_derivative = cross_entropy_derivative\n",
        "\n",
        "        else:\n",
        "          raise ValueError(f\"Unsupported Loss Function: {loss_function}\")\n",
        "\n",
        "\n",
        "    def backward(self, dLoss):\n",
        "        for layer in reversed(self.layers):\n",
        "            dLoss = layer.pass_backwards(dLoss)\n",
        "\n",
        "    def train(self, x, y_true, epochs=20, batch_size = 100, print_every=1):\n",
        "        if self.optimizer == 'sgd':\n",
        "            for epoch in range(epochs):\n",
        "                for start in range(0, len(x[0]), batch_size):  # Process in batches of 100 samples\n",
        "                    x_batch = x[:, start:start+batch_size]  # Shape: (784, 100)\n",
        "                    y_batch = y_true[:, start:start+batch_size]  # Shape: (10, 100)\n",
        "\n",
        "                    # Forward pass\n",
        "                    y_pred = self.forward(x_batch)  # Shape: (10, 100)\n",
        "\n",
        "                    # Compute the loss for the entire batch\n",
        "                    loss = self.loss_function(y_batch, y_pred)  # Shape: (10, 100)\n",
        "                    dLoss = self.loss_function_derivative(y_batch, y_pred)\n",
        "\n",
        "                    # Backward pass\n",
        "                    self.backward(dLoss)\n",
        "\n",
        "                if epoch % print_every == 0 or epoch == epochs - 1:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer: {self.optimizer}\")\n",
        "\n",
        "    def reset_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.reset_weights()\n",
        "\n",
        "    def predict(self, x):\n",
        "      return self.forward(x)\n",
        "\n",
        "    def evaluate(self,x_test, y_test):\n",
        "        correct = 0\n",
        "        for i in range(len(x_test[0])):\n",
        "            current_image = x_test[:, i, None]\n",
        "            prediction = self.predict(current_image)\n",
        "            label = y_test[i]\n",
        "            if np.argmax(prediction) == label:\n",
        "                correct += 1\n",
        "        return correct / len(x_test[0]) #Finding the average score of the values\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SpQZRAjiMKlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "model.add(ModelLayer(128, activation='relu'))\n",
        "model.add(ModelLayer(64, activation='relu'))\n",
        "model.add(ModelLayer(32, activation='relu'))\n",
        "model.add(ModelLayer(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=\"sgd\", loss_function=\"cross_entropy\")\n",
        "\n",
        "# Force layer build by passing in input shape\n",
        "_ = model.forward(x_train[:, :1])\n",
        "model.reset_weights()\n",
        "\n",
        "model.train(x_train[:, :2000], y_train_encoded[:, :2000], epochs=50, print_every=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzGHlhhImegv",
        "outputId": "3d42a02a-fedb-478b-825f-205ee65e45ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 124.3961\n",
            "Epoch 10, Loss: 11.1877\n",
            "Epoch 20, Loss: 4.1599\n",
            "Epoch 30, Loss: 1.8390\n",
            "Epoch 40, Loss: 0.9485\n",
            "Epoch 49, Loss: 0.6012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def test_prediction(index):\n",
        "    current_image = x_test[:, index, None]\n",
        "    prediction = model.predict(current_image)\n",
        "    label = y_test[index]\n",
        "\n",
        "\n",
        "    print(\"Model Output: \\n\", prediction)\n",
        "    print(\"Prediction: \", np.argmax(prediction))\n",
        "    print(\"Label: \", label)\n",
        "\n",
        "    current_image = current_image.reshape((28, 28)) * 255\n",
        "    plt.gray()\n",
        "    plt.imshow(current_image, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "test_prediction(random.randint(0,len(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "fWiUOywl-xNr",
        "outputId": "48ea40ef-efad-4c84-d54e-60fc2a760ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output: \n",
            " [[1.35066060e-10]\n",
            " [9.99982961e-01]\n",
            " [2.82404498e-08]\n",
            " [5.90270649e-08]\n",
            " [4.17962946e-06]\n",
            " [5.36897510e-07]\n",
            " [5.50151986e-08]\n",
            " [8.72948886e-06]\n",
            " [3.44869935e-06]\n",
            " [2.04862295e-09]]\n",
            "Prediction:  1\n",
            "Label:  1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGaNJREFUeJzt3X9M1Pcdx/HXoXLaFo4hwkFFi9pqUyvLUBlRmY1EYIvxVxPbNYsuTqPDZuraLpip7bqMzSVd08Vp/9I1q7YzmZqaxUWxYLqhjaIxbpWIYwMjYDXhDrGikc/+ML31KqgHd7w5fD6ST1Lu+/1wn373rc99uS9fPc45JwAA+lmC9QIAAA8nAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwMtV7A13V1denSpUtKSkqSx+OxXg4AIELOObW3tysrK0sJCT1f5wy4AF26dEnZ2dnWywAA9FFTU5NGjx7d4/YB9yO4pKQk6yUAAKLgfn+exyxAW7du1RNPPKHhw4crPz9fn3766QPN48duADA43O/P85gE6MMPP9T69eu1efNm1dbWKjc3V8XFxbp8+XIs3g4AEI9cDEyfPt2VlZWFvr59+7bLyspyFRUV950bCAScJAaDwWDE+QgEAvf88z7qV0A3b97UyZMnVVRUFHotISFBRUVFqqmpuWv/zs5OBYPBsAEAGPyiHqArV67o9u3bysjICHs9IyNDLS0td+1fUVEhn88XGtwBBwAPB/O74MrLyxUIBEKjqanJekkAgH4Q9d8DSktL05AhQ9Ta2hr2emtrq/x+/137e71eeb3eaC8DADDARf0KKDExUXl5eaqsrAy91tXVpcrKShUUFET77QAAcSomT0JYv369li5dqqlTp2r69Ol6++231dHRoR/+8IexeDsAQByKSYCWLFmizz//XJs2bVJLS4u++c1v6uDBg3fdmAAAeHh5nHPOehFfFQwG5fP5rJcBAOijQCCg5OTkHreb3wUHAHg4ESAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACaGWi8AiHd5eXkRz/nrX/8a8ZxPPvkk4jk/+MEPIp4jSdevX+/VPCASXAEBAEwQIACAiagH6PXXX5fH4wkbkyZNivbbAADiXEw+A3rmmWd0+PDh/7/JUD5qAgCEi0kZhg4dKr/fH4tvDQAYJGLyGdD58+eVlZWlcePG6aWXXlJjY2OP+3Z2dioYDIYNAMDgF/UA5efna+fOnTp48KC2bdumhoYGzZo1S+3t7d3uX1FRIZ/PFxrZ2dnRXhIAYADyOOdcLN+gra1NY8eO1VtvvaXly5fftb2zs1OdnZ2hr4PBIBFCXOH3gIDuBQIBJScn97g95ncHpKSk6KmnnlJ9fX23271er7xeb6yXAQAYYGL+e0DXrl3ThQsXlJmZGeu3AgDEkagH6JVXXlF1dbX+85//6B//+IcWLlyoIUOG6MUXX4z2WwEA4ljUfwR38eJFvfjii7p69apGjRqlmTNn6tixYxo1alS03woAEMdifhNCpILBoHw+n/UygAe2ffv2iOf86Ec/iniOx+OJeM6vfvWriOdI0saNG3s1D/iq+92EwLPgAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATMf8L6QDcrTcPFu3NnLS0tIjnAP2FKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GnYQB89/fTTEc9xzsVgJUB84QoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBw0iBPpo1a1bEc3rzMFKPxxPxHGAg4woIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBw0iBPurNg0V7M6c3zp071y/vA/QGV0AAABMECABgIuIAHT16VPPmzVNWVpY8Ho/27dsXtt05p02bNikzM1MjRoxQUVGRzp8/H631AgAGiYgD1NHRodzcXG3durXb7Vu2bNE777yj7du36/jx43r00UdVXFysGzdu9HmxAIDBI+KbEEpLS1VaWtrtNuec3n77bf385z/X/PnzJUnvvfeeMjIytG/fPr3wwgt9Wy0AYNCI6mdADQ0NamlpUVFRUeg1n8+n/Px81dTUdDuns7NTwWAwbAAABr+oBqilpUWSlJGREfZ6RkZGaNvXVVRUyOfzhUZ2dnY0lwQAGKDM74IrLy9XIBAIjaamJuslAQD6QVQD5Pf7JUmtra1hr7e2toa2fZ3X61VycnLYAAAMflENUE5Ojvx+vyorK0OvBYNBHT9+XAUFBdF8KwBAnIv4Lrhr166pvr4+9HVDQ4NOnz6t1NRUjRkzRmvXrtUvf/lLPfnkk8rJydHGjRuVlZWlBQsWRHPdAIA4F3GATpw4oeeeey709fr16yVJS5cu1c6dO/Xaa6+po6NDK1euVFtbm2bOnKmDBw9q+PDh0Vs1ACDueVx/PRXxAQWDQfl8PutlAA+sq6sr4jm9+c/O4/FEPGfq1KkRz5Gk2traXs0DvioQCNzzc33zu+AAAA8nAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmIj4r2MAEK43T7YeYA+hB0xwBQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOBhpEAfeTyefnmf2trafpkD9BeugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEzyMFOgj51y/zAEGG66AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATPIwU6COPx2O9BCAucQUEADBBgAAAJiIO0NGjRzVv3jxlZWXJ4/Fo3759YduXLVsmj8cTNkpKSqK1XgDAIBFxgDo6OpSbm6utW7f2uE9JSYmam5tDY/fu3X1aJABg8In4JoTS0lKVlpbecx+v1yu/39/rRQEABr+YfAZUVVWl9PR0TZw4UatXr9bVq1d73Lezs1PBYDBsAAAGv6gHqKSkRO+9954qKyv1m9/8RtXV1SotLdXt27e73b+iokI+ny80srOzo70kAMAA5HHOuV5P9ni0d+9eLViwoMd9/v3vf2v8+PE6fPiw5syZc9f2zs5OdXZ2hr4OBoNECHGlq6sr4jm9+c+utrY24jnTpk2LeA4QLYFAQMnJyT1uj/lt2OPGjVNaWprq6+u73e71epWcnBw2AACDX8wDdPHiRV29elWZmZmxfisAQByJ+C64a9euhV3NNDQ06PTp00pNTVVqaqreeOMNLV68WH6/XxcuXNBrr72mCRMmqLi4OKoLBwDEt4gDdOLECT333HOhr9evXy9JWrp0qbZt26YzZ87oj3/8o9ra2pSVlaW5c+fqzTfflNfrjd6qAQBxL+IAzZ49+54foP7tb3/r04KAeNObGwr6cO8PMGjwLDgAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYiPhp2ADCeTwe6yUAcYkrIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABA8jBfrIOdcvc4DBhisgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEDyMF+sjj8VgvAYhLXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZ4GCnQR5999lnEcyZOnBjxnLS0tH6ZI0lXrlzp1TwgElwBAQBMECAAgImIAlRRUaFp06YpKSlJ6enpWrBggerq6sL2uXHjhsrKyjRy5Eg99thjWrx4sVpbW6O6aABA/IsoQNXV1SorK9OxY8d06NAh3bp1S3PnzlVHR0don3Xr1umjjz7Snj17VF1drUuXLmnRokVRXzgAIL55nHOut5M///xzpaenq7q6WoWFhQoEAho1apR27dql559/XpJ07tw5Pf3006qpqdG3v/3t+37PYDAon8/X2yUB/e6f//xnxHN6cxNCU1NTxHOmTZsW8RyJmxAQHYFAQMnJyT1u79NnQIFAQJKUmpoqSTp58qRu3bqloqKi0D6TJk3SmDFjVFNT0+336OzsVDAYDBsAgMGv1wHq6urS2rVrNWPGDE2ePFmS1NLSosTERKWkpITtm5GRoZaWlm6/T0VFhXw+X2hkZ2f3dkkAgDjS6wCVlZXp7Nmz+uCDD/q0gPLycgUCgdDozY8ZAADxp1e/iLpmzRodOHBAR48e1ejRo0Ov+/1+3bx5U21tbWFXQa2trfL7/d1+L6/XK6/X25tlAADiWERXQM45rVmzRnv37tWRI0eUk5MTtj0vL0/Dhg1TZWVl6LW6ujo1NjaqoKAgOisGAAwKEV0BlZWVadeuXdq/f7+SkpJCn+v4fD6NGDFCPp9Py5cv1/r165Wamqrk5GS9/PLLKigoeKA74AAAD4+IArRt2zZJ0uzZs8Ne37Fjh5YtWyZJ+t3vfqeEhAQtXrxYnZ2dKi4u1h/+8IeoLBYAMHj06feAYoHfA0K8efPNNyOes2HDhojnJCREfs9QXl5exHMkqba2tlfzgK+K6e8BAQDQWwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDRq78RFcD/7du3L+I55eXlEc/p6uqKeA4wkHEFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GGkQB81NjZGPOfUqVMRz5k6dWrEczZs2BDxHEl6/vnnezUPiARXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACY9zzlkv4quCwaB8Pp/1MoCYmjlzZsRz3n333YjnTJw4MeI5kjR0KM8pRt8FAgElJyf3uJ0rIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABA8jBQDEBA8jBQAMSAQIAGAiogBVVFRo2rRpSkpKUnp6uhYsWKC6urqwfWbPni2PxxM2Vq1aFdVFAwDiX0QBqq6uVllZmY4dO6ZDhw7p1q1bmjt3rjo6OsL2W7FihZqbm0Njy5YtUV00ACD+RfTXHh48eDDs6507dyo9PV0nT55UYWFh6PVHHnlEfr8/OisEAAxKffoMKBAISJJSU1PDXn///feVlpamyZMnq7y8XNevX+/xe3R2dioYDIYNAMBDwPXS7du33fe+9z03Y8aMsNffffddd/DgQXfmzBn3pz/9yT3++ONu4cKFPX6fzZs3O0kMBoPBGGQjEAjcsyO9DtCqVavc2LFjXVNT0z33q6ysdJJcfX19t9tv3LjhAoFAaDQ1NZkfNAaDwWD0fdwvQBF9BvSlNWvW6MCBAzp69KhGjx59z33z8/MlSfX19Ro/fvxd271er7xeb2+WAQCIYxEFyDmnl19+WXv37lVVVZVycnLuO+f06dOSpMzMzF4tEAAwOEUUoLKyMu3atUv79+9XUlKSWlpaJEk+n08jRozQhQsXtGvXLn33u9/VyJEjdebMGa1bt06FhYWaMmVKTP4FAABxKpLPfdTDz/l27NjhnHOusbHRFRYWutTUVOf1et2ECRPcq6++et+fA35VIBAw/7klg8FgMPo+7vdnPw8jBQDEBA8jBQAMSAQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwMuQM456yUAAKLgfn+eD7gAtbe3Wy8BABAF9/vz3OMG2CVHV1eXLl26pKSkJHk8nrBtwWBQ2dnZampqUnJystEK7XEc7uA43MFxuIPjcMdAOA7OObW3tysrK0sJCT1f5wztxzU9kISEBI0ePfqe+yQnJz/UJ9iXOA53cBzu4DjcwXG4w/o4+Hy+++4z4H4EBwB4OBAgAICJuAqQ1+vV5s2b5fV6rZdiiuNwB8fhDo7DHRyHO+LpOAy4mxAAAA+HuLoCAgAMHgQIAGCCAAEATBAgAICJuAnQ1q1b9cQTT2j48OHKz8/Xp59+ar2kfvf666/L4/GEjUmTJlkvK+aOHj2qefPmKSsrSx6PR/v27Qvb7pzTpk2blJmZqREjRqioqEjnz5+3WWwM3e84LFu27K7zo6SkxGaxMVJRUaFp06YpKSlJ6enpWrBggerq6sL2uXHjhsrKyjRy5Eg99thjWrx4sVpbW41WHBsPchxmz5591/mwatUqoxV3Ly4C9OGHH2r9+vXavHmzamtrlZubq+LiYl2+fNl6af3umWeeUXNzc2h88skn1kuKuY6ODuXm5mrr1q3dbt+yZYveeecdbd++XcePH9ejjz6q4uJi3bhxo59XGlv3Ow6SVFJSEnZ+7N69ux9XGHvV1dUqKyvTsWPHdOjQId26dUtz585VR0dHaJ9169bpo48+0p49e1RdXa1Lly5p0aJFhquOvgc5DpK0YsWKsPNhy5YtRivugYsD06dPd2VlZaGvb9++7bKyslxFRYXhqvrf5s2bXW5urvUyTElye/fuDX3d1dXl/H6/++1vfxt6ra2tzXm9Xrd7926DFfaPrx8H55xbunSpmz9/vsl6rFy+fNlJctXV1c65O//bDxs2zO3Zsye0z2effeYkuZqaGqtlxtzXj4Nzzn3nO99xP/nJT+wW9QAG/BXQzZs3dfLkSRUVFYVeS0hIUFFRkWpqagxXZuP8+fPKysrSuHHj9NJLL6mxsdF6SaYaGhrU0tISdn74fD7l5+c/lOdHVVWV0tPTNXHiRK1evVpXr161XlJMBQIBSVJqaqok6eTJk7p161bY+TBp0iSNGTNmUJ8PXz8OX3r//feVlpamyZMnq7y8XNevX7dYXo8G3MNIv+7KlSu6ffu2MjIywl7PyMjQuXPnjFZlIz8/Xzt37tTEiRPV3NysN954Q7NmzdLZs2eVlJRkvTwTLS0tktTt+fHltodFSUmJFi1apJycHF24cEEbNmxQaWmpampqNGTIEOvlRV1XV5fWrl2rGTNmaPLkyZLunA+JiYlKSUkJ23cwnw/dHQdJ+v73v6+xY8cqKytLZ86c0c9+9jPV1dXpL3/5i+Fqww34AOH/SktLQ/88ZcoU5efna+zYsfrzn/+s5cuXG64MA8ELL7wQ+udnn31WU6ZM0fjx41VVVaU5c+YYriw2ysrKdPbs2Yfic9B76ek4rFy5MvTPzz77rDIzMzVnzhxduHBB48eP7+9ldmvA/wguLS1NQ4YMuesultbWVvn9fqNVDQwpKSl66qmnVF9fb70UM1+eA5wfdxs3bpzS0tIG5fmxZs0aHThwQB9//HHYX9/i9/t18+ZNtbW1he0/WM+Hno5Dd/Lz8yVpQJ0PAz5AiYmJysvLU2VlZei1rq4uVVZWqqCgwHBl9q5du6YLFy4oMzPTeilmcnJy5Pf7w86PYDCo48ePP/Tnx8WLF3X16tVBdX4457RmzRrt3btXR44cUU5OTtj2vLw8DRs2LOx8qKurU2Nj46A6H+53HLpz+vRpSRpY54P1XRAP4oMPPnBer9ft3LnT/etf/3IrV650KSkprqWlxXpp/eqnP/2pq6qqcg0NDe7vf/+7Kyoqcmlpae7y5cvWS4up9vZ2d+rUKXfq1Cknyb311lvu1KlT7r///a9zzrlf//rXLiUlxe3fv9+dOXPGzZ8/3+Xk5LgvvvjCeOXRda/j0N7e7l555RVXU1PjGhoa3OHDh923vvUt9+STT7obN25YLz1qVq9e7Xw+n6uqqnLNzc2hcf369dA+q1atcmPGjHFHjhxxJ06ccAUFBa6goMBw1dF3v+NQX1/vfvGLX7gTJ064hoYGt3//fjdu3DhXWFhovPJwcREg55z7/e9/78aMGeMSExPd9OnT3bFjx6yX1O+WLFniMjMzXWJionv88cfdkiVLXH19vfWyYu7jjz92ku4aS5cudc7duRV748aNLiMjw3m9XjdnzhxXV1dnu+gYuNdxuH79ups7d64bNWqUGzZsmBs7dqxbsWLFoPs/ad39+0tyO3bsCO3zxRdfuB//+MfuG9/4hnvkkUfcwoULXXNzs92iY+B+x6GxsdEVFha61NRU5/V63YQJE9yrr77qAoGA7cK/hr+OAQBgYsB/BgQAGJwIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABP/A6pOZNr++44VAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y92cpGbQTlV6",
        "outputId": "ce8153c9-46fd-46f5-8cbb-652f65bf551c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9018"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ls_eDgnMS5iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTcBRyPXQoX2",
        "outputId": "56b850e3-3a57-4b57-fa2a-6d7269a99e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 1. 1.]\n",
            " [1. 1. 3.]\n",
            " [1. 3. 1.]]\n"
          ]
        }
      ]
    }
  ]
}