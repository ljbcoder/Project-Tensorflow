{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC9OWubeonXrtI4Z1Y3xgR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljbcoder/Project-Tensorflow/blob/main/Project_Tensorflow_AI_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What's Going on: Directly Under the Hood\n",
        "import numpy as np\n",
        "\n",
        "I = np.array([[0.05,.10]])\n",
        "T = np.array([[0,1]])\n",
        "WH = np.array([[0.15, 0.25],\n",
        "               [0.2,0.3]])\n",
        "\n",
        "BH = np.array([[0.35,0.35]])\n",
        "WO = np.array([[0.4, 0.5],\n",
        "               [0.45,0.55]])\n",
        "BO = np.array([[0.6,.6]])\n",
        "\n",
        "for epoch in range(100000):\n",
        "  print(epoch)\n",
        "\n",
        "  #Apply sigmoid to first layer\n",
        "  H = I @ WH + BH\n",
        "  H = 1/(1+np.exp(-H))\n",
        "\n",
        "  #Apply Softmax for second layer\n",
        "  O = H @ WO + BO\n",
        "  OM = O - np.max(O)\n",
        "  O = np.exp(OM)/np.sum(np.exp(OM))\n",
        "\n",
        "  E = np.sum(-T*np.log(O))\n",
        "  if E < 0.0001:\n",
        "    break\n",
        "\n",
        "  Ob = O - T\n",
        "  #Nothing for softmax cross entropy error\n",
        "\n",
        "  Hb = Ob @ WO.T\n",
        "  Hb = Hb * H * (1-H)\n",
        "\n",
        "  WHb = I.T @ Hb\n",
        "  BHb = 1* Hb\n",
        "  WOb = H.T @ Ob\n",
        "  BOb = 1 * Ob\n",
        "\n",
        "  lr = 0.01\n",
        "  WH = WH - lr * WHb\n",
        "  BH = BH - lr * BHb\n",
        "  WO = WO - lr * WOb\n",
        "  BO = BO - lr - BOb\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mi5kWcIWqHHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINAL CODE**"
      ],
      "metadata": {
        "id": "8f-QwhcYCjQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "ww4fmNlykgMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train.reshape(-1, 28*28).T / 255.0  # shape: (784, num_samples)\n",
        "x_test = x_test.reshape(-1, 28*28).T / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train).T  # shape: (10, num_samples)\n",
        "y_test = to_categorical(y_test).T\n"
      ],
      "metadata": {
        "id": "p9ctQJqIkoFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Each image corresponds to a column of x --> 28x28 pixels / 60000 images\n",
        "#Each Label corresponds to a column of y --> 10 categories / 60000 labels\n",
        "print(\"Shape of x_train: \",x_train.shape)\n",
        "print(\"Shape of y_train: \",y_train.shape)\n",
        "print(\"Shape of x_test: \",x_test.shape)\n",
        "print(\"Shape of y_test: \",y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKEiSHvUezuU",
        "outputId": "09d30f74-dcb4-42f5-d043-438a88788f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_train:  (784, 60000)\n",
            "Shape of y_train:  (10, 60000)\n",
            "Shape of x_test:  (784, 10000)\n",
            "Shape of y_test:  (10, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "#I put Softmax derivative as implicit: Cancels out with Cross entropy\n",
        "# def softmax_derivative(z):\n",
        "#     s = softmax(z)\n",
        "#     return s * (1 - s) --> This eventually cancels out with 1/s multiplied in the backpropagation step\n",
        "\n",
        "#Loss Functions\n",
        "def MSE(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def MSE_derivative(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true)\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    epsilon = 1e-10  # Small value to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Clip values to avoid log(0)\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "\n",
        "def cross_entropy_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true\n",
        "\n",
        "class ModelLayer:\n",
        "    def __init__(self, output_size, activation='relu', input_size=None, learning_rate=0.001):\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size  # Set later if None\n",
        "        self.activation_name = activation.lower()\n",
        "        self.activation = None\n",
        "        self.activation_derivative = None\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights = None  # Will be initialized when input_size is known\n",
        "        self.bias = None\n",
        "\n",
        "        self.input_vals = None\n",
        "        self.z = None\n",
        "\n",
        "    def build(self, input_size):\n",
        "        \"\"\"Initialize weights and biases once input size is known\"\"\"\n",
        "        self.input_size = input_size\n",
        "\n",
        "        if self.activation_name == 'relu':\n",
        "            # He initialization (Used mostly for relu, leaky relu) --> Weight initializations to avoid vanishing/exploding gradient\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = relu\n",
        "            self.activation_derivative = relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'leaky_relu':\n",
        "            # He initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = leaky_relu\n",
        "            self.activation_derivative = leaky_relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'sigmoid':\n",
        "            # Xavier initialization -> (Used mostly for Sigmoid, tanh, softmax) --> Weight initializations to avoid vanishing/exploding gradient\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(1. / input_size)\n",
        "            self.activation = sigmoid\n",
        "            self.activation_derivative = sigmoid_derivative\n",
        "\n",
        "        elif self.activation_name == 'softmax':\n",
        "            # Xavier initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(1. / input_size)\n",
        "            self.activation = softmax\n",
        "            self.activation_derivative = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {self.activation_name}\")\n",
        "\n",
        "        self.bias = np.zeros((self.output_size, 1))\n",
        "\n",
        "    #Forward Propagation\n",
        "    def pass_forward(self, input_vals):\n",
        "        if self.weights is None:\n",
        "            self.build(input_vals.shape[0])\n",
        "        self.input_vals = input_vals\n",
        "        self.z = np.dot(self.weights, input_vals) + self.bias\n",
        "        return self.activation(self.z)\n",
        "\n",
        "    #Backpropagation\n",
        "    def pass_backwards(self, dA):\n",
        "        if self.activation_name == 'softmax':\n",
        "            dZ = dA  # Softmax derivative is handled implicitly in cross-entropy\n",
        "        else:\n",
        "          dZ = dA * self.activation_derivative(self.z)\n",
        "\n",
        "        dW = np.dot(dZ, self.input_vals.T)\n",
        "        dB = np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = np.dot(self.weights.T, dZ)\n",
        "\n",
        "        self.weights -= self.learning_rate * dW\n",
        "        self.bias -= self.learning_rate * dB\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "    def reset_weights(self):\n",
        "        self.build(self.input_size)\n",
        "\n",
        "class Model:\n",
        "    def __init__(self,layers = []):\n",
        "        self.layers = layers\n",
        "        self.built = False\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    #Builds layers after all layer_sizes have been set\n",
        "    def _build_layers(self, input_size):\n",
        "        for layer in self.layers:\n",
        "            if layer.input_size is None:\n",
        "                layer.build(input_size)\n",
        "            input_size = layer.output_size  # next layer's input\n",
        "\n",
        "    #Set optimizers and loss functions\n",
        "    def compile(self, optimizer, loss_function):\n",
        "        self.optimizer = optimizer.lower()\n",
        "\n",
        "        self.loss_function_name = loss_function.lower()\n",
        "\n",
        "        #Apply the loss function\n",
        "        if self.loss_function_name == 'mse':\n",
        "          self.loss_function = MSE\n",
        "          self.loss_function_derivative = MSE_derivative\n",
        "\n",
        "        elif self.loss_function_name == 'cross_entropy':\n",
        "          self.loss_function = cross_entropy\n",
        "          self.loss_function_derivative = cross_entropy_derivative\n",
        "\n",
        "        else:\n",
        "          raise ValueError(f\"Unsupported Loss Function: {loss_function}\")\n",
        "\n",
        "    #Forward Propagation\n",
        "    def forward(self, x):\n",
        "        if not self.built:\n",
        "            self._build_layers(x.shape[0])\n",
        "            self.built = True\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.pass_forward(x)\n",
        "        return x\n",
        "\n",
        "    #Backpropagation\n",
        "    def backward(self, dLoss):\n",
        "        for layer in reversed(self.layers):\n",
        "            dLoss = layer.pass_backwards(dLoss)\n",
        "\n",
        "    #Train the model!\n",
        "    def train(self, x, y_true, epochs=20, batch_size = 100, print_every=1):\n",
        "        if self.optimizer == 'sgd':\n",
        "            for epoch in range(epochs):\n",
        "                for start in range(0, len(x[0]), batch_size):  # Process in batches of 100 samples\n",
        "                    x_batch = x[:, start:start+batch_size]  # Shape: (784, 100)\n",
        "                    y_batch = y_true[:, start:start+batch_size]  # Shape: (10, 100)\n",
        "\n",
        "                    # Forward pass\n",
        "                    y_pred = self.forward(x_batch)  # Shape: (10, 100)\n",
        "\n",
        "                    # Compute the loss for the entire batch\n",
        "                    loss = self.loss_function(y_batch, y_pred)  # Shape: (10, 100)\n",
        "                    dLoss = self.loss_function_derivative(y_batch, y_pred)\n",
        "\n",
        "                    # Backward pass\n",
        "                    self.backward(dLoss)\n",
        "\n",
        "                if epoch % print_every == 0 or epoch == epochs - 1:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer: {self.optimizer}\")\n",
        "\n",
        "    def reset_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.reset_weights()\n",
        "\n",
        "    #Predictions are just forward propagations of a given input\n",
        "    def predict(self, x):\n",
        "      return self.forward(x)\n",
        "\n",
        "    #Evaluate the model on test data\n",
        "    def evaluate(self,x_test, y_test):\n",
        "        correct = 0\n",
        "        for i in range(len(x_test[0])):\n",
        "            current_image = x_test[:, i, None]\n",
        "            prediction = self.predict(current_image)\n",
        "            label = y_test[i]\n",
        "            if np.argmax(prediction) == label:\n",
        "                correct += 1\n",
        "        return correct / len(x_test[0]) #Finding the average score of the values\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SpQZRAjiMKlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "model.add(ModelLayer(128, activation='relu'))\n",
        "model.add(ModelLayer(64, activation='relu'))\n",
        "model.add(ModelLayer(32, activation='relu'))\n",
        "model.add(ModelLayer(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=\"sgd\", loss_function=\"cross_entropy\")\n",
        "model.train(x_train[:, :2000], y_train[:, :2000], epochs=50, print_every=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzGHlhhImegv",
        "outputId": "5c45d25d-ed38-446d-b6e6-29d10e36e1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 153.2372\n",
            "Epoch 10, Loss: 13.4542\n",
            "Epoch 20, Loss: 3.5602\n",
            "Epoch 30, Loss: 1.4509\n",
            "Epoch 40, Loss: 0.7617\n",
            "Epoch 49, Loss: 0.4989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def test_prediction(index):\n",
        "    current_image = x_test[:, index, None]\n",
        "    prediction = model.predict(current_image)\n",
        "    label = y_test[index]\n",
        "\n",
        "\n",
        "    print(\"Model Output: \\n\", prediction)\n",
        "    print(\"Prediction: \", np.argmax(prediction))\n",
        "    print(\"Label: \", label)\n",
        "\n",
        "    current_image = current_image.reshape((28, 28)) * 255\n",
        "    plt.gray()\n",
        "    plt.imshow(current_image, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "test_prediction(random.randint(0,len(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "fWiUOywl-xNr",
        "outputId": "48ea40ef-efad-4c84-d54e-60fc2a760ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output: \n",
            " [[1.35066060e-10]\n",
            " [9.99982961e-01]\n",
            " [2.82404498e-08]\n",
            " [5.90270649e-08]\n",
            " [4.17962946e-06]\n",
            " [5.36897510e-07]\n",
            " [5.50151986e-08]\n",
            " [8.72948886e-06]\n",
            " [3.44869935e-06]\n",
            " [2.04862295e-09]]\n",
            "Prediction:  1\n",
            "Label:  1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y92cpGbQTlV6",
        "outputId": "ce8153c9-46fd-46f5-8cbb-652f65bf551c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9018"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    }
  ]
}