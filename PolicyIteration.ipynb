{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMshum31SrA90u0j4ujMANv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljbcoder/Project-Tensorflow/blob/main/PolicyIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpShcz3_Ok6j",
        "outputId": "7d644966-b646-4f94-8241-42cabe579c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Evaluation took 173 iterations\n",
            "[[  0.         -13.99893866 -19.99842728 -21.99824003]\n",
            " [-13.99893866 -17.99861452 -19.9984378  -19.99842728]\n",
            " [-19.99842728 -19.9984378  -17.99861452 -13.99893866]\n",
            " [-21.99824003 -19.99842728 -13.99893866   0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "grid_size = 4\n",
        "states = grid_size **2\n",
        "actions = [\"up\",\"down\",\"left\", \"right\"]\n",
        "gamma = 1.0\n",
        "theta = 1e-4\n",
        "\n",
        "#Conditions for the grid world scenario\n",
        "def step(state, action):\n",
        "    row, col = divmod(state, grid_size) #Returns the quotient, remainder of state / grid_size\n",
        "    if state in [0,15]:\n",
        "      return state,0\n",
        "\n",
        "    if action == \"up\":\n",
        "      row = max(row-1,0)\n",
        "    if action == \"down\":\n",
        "      row = min(row+1,grid_size-1)\n",
        "    if action == \"left\":\n",
        "      col = max(col-1,0)\n",
        "    if action == \"right\":\n",
        "      col = min(col+1,grid_size-1)\n",
        "\n",
        "    next_state = row * grid_size + col\n",
        "    reward = -1\n",
        "    return next_state, reward\n",
        "\n",
        "#All possible probabilities for taking an action (For each state)\n",
        "policy = {s: {a:0.25 for a in actions} for s in range(states)}  #Uniform random policy\n",
        "\n",
        "#Initially start off with all zeros\n",
        "V = np.zeros(states)\n",
        "\n",
        "\n",
        "#Iterative Policy Evaluation\n",
        "iteration = 0\n",
        "\n",
        "while True:\n",
        "  delta = 0\n",
        "  new_V = np.copy(V)\n",
        "\n",
        "  for s in range(states):\n",
        "    #Calculate Value Function (@ State s)\n",
        "    # Value function = sum(policy * sum(probability* (reward +  * gamma * value_function)))\n",
        "\n",
        "    v= 0\n",
        "    for a in actions:\n",
        "      next_state, reward = step(s,a)\n",
        "      v += policy[s][a] * (reward + gamma * V[next_state]) #We got rid of probability of going to next action because all 1\n",
        "\n",
        "    #Alternative method to calculate V[s]:\n",
        "    # V[s] = sum(policy[s][a] * (step(s,a)[1] + gamma * V[step(s,a)[0]]) for a in actions)\n",
        "    # delta = max(delta, abs(v-V[s]))\n",
        "\n",
        "    #Save new values into new_V\n",
        "    new_V[s] = v\n",
        "    delta = max(delta, abs(v-V[s]))\n",
        "\n",
        "  V = new_V\n",
        "  iteration += 1\n",
        "\n",
        "  if delta < theta:\n",
        "    break\n",
        "\n",
        "\n",
        "print(f\"Policy Evaluation took {iteration} iterations\")\n",
        "print(V.reshape(grid_size,grid_size))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "grid_size = 4\n",
        "states = grid_size **2\n",
        "actions = [\"up\",\"down\",\"left\", \"right\"]\n",
        "gamma = 1.0  #Discount Factor\n",
        "theta = 1e-4  #Error Bound\n",
        "\n",
        "#Conditions for the grid world scenario\n",
        "def step(state, action):\n",
        "    row, col = divmod(state, grid_size) #Returns the quotient, remainder of state / grid_size\n",
        "    if state in [0,15]:\n",
        "      return state,0\n",
        "\n",
        "    if action == \"up\":\n",
        "      row = max(row-1,0)\n",
        "    if action == \"down\":\n",
        "      row = min(row+1,grid_size-1)\n",
        "    if action == \"left\":\n",
        "      col = max(col-1,0)\n",
        "    if action == \"right\":\n",
        "      col = min(col+1,grid_size-1)\n",
        "\n",
        "    next_state = row * grid_size + col\n",
        "    reward = -1\n",
        "    return next_state, reward\n",
        "\n",
        "#All possible probabilities for taking an action (For each state)\n",
        "policy = {s: {a:0.25 for a in actions} for s in range(states)}  #Uniform random policy\n",
        "\n",
        "#Initially start off with all zeros\n",
        "V = np.zeros(states)\n",
        "\n",
        "\n",
        "\n",
        "iteration = 0\n",
        "policy_stable = False\n",
        "\n",
        "while not policy_stable:\n",
        "\n",
        "  #Iterative Policy Evaluation\n",
        "  while True:\n",
        "    delta = 0\n",
        "    new_V = np.copy(V)\n",
        "\n",
        "    for s in range(states):\n",
        "      #Calculate Value Function (@ State s)\n",
        "      # Value function = sum(policy * sum(probability* (reward +  * gamma * value_function)))\n",
        "\n",
        "      v= 0\n",
        "      for a in actions:\n",
        "        next_state, reward = step(s,a)\n",
        "        v += policy[s][a] * (reward + gamma * V[next_state]) #We got rid of probability of going to next action because all 1\n",
        "\n",
        "      #Alternative method to calculate V[s]:\n",
        "      # V[s] = sum(policy[s][a] * (step(s,a)[1] + gamma * V[step(s,a)[0]]) for a in actions)\n",
        "      # delta = max(delta, abs(v-V[s]))\n",
        "\n",
        "      #Save new values into new_V\n",
        "      new_V[s] = v\n",
        "      delta = max(delta, abs(v-V[s]))\n",
        "\n",
        "    V = new_V\n",
        "\n",
        "    if delta < theta:\n",
        "      break\n",
        "\n",
        "  print(f\"Iteration {iteration}: Value Function\")\n",
        "  print(V.reshape(grid_size,grid_size))\n",
        "  print()\n",
        "\n",
        "  print(\"Current Policy:\")\n",
        "  for s in range(states):\n",
        "      if s in [0, 15]:\n",
        "          print(f\"State {s}: Terminal\")\n",
        "      else:\n",
        "          # best_action = max(policy[s], key=policy[s].get)\n",
        "          # print(f\"State {s}: {best_action}\")\n",
        "          print(f\"State {s}: {policy[s]}\")\n",
        "\n",
        "  for i in range(20):\n",
        "    print(\"___\",end=\"\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "  #Policy Improvement\n",
        "  policy_stable = True\n",
        "  for s in range(states):\n",
        "\n",
        "    #Skip over states 0 and 15 (end-points)\n",
        "    if s in [0,15]:\n",
        "      continue\n",
        "\n",
        "    old_action = max(policy[s], key=policy[s].get) #Get the highest probability action\n",
        "\n",
        "    action_values = {}\n",
        "    for a in actions:\n",
        "      next_state, reward = step(s,a)\n",
        "      action_values[a] = reward + gamma * V[next_state] #Compute q_pi (Action Value function)\n",
        "\n",
        "    #Single best action to take:\n",
        "    #best_action = max(action_values, key=action_values.get)\n",
        "\n",
        "    #All best actions:\n",
        "    best_value = max(action_values.values())\n",
        "    best_actions = [a for a, val in action_values.items() if val == best_value]\n",
        "\n",
        "\n",
        "    #Greedy policy: Probabilities for choosing action\n",
        "    new_policy = {a: (1.0 / len(best_actions) if a in best_actions else 0.0) for a in actions}\n",
        "\n",
        "\n",
        "    #Check if policy changed\n",
        "    if new_policy != policy[s]:\n",
        "      policy_stable = False\n",
        "\n",
        "    #Note that new_policy is just the new policy for a given state s, we need to iterate through all states\n",
        "    policy[s] = new_policy\n",
        "\n",
        "  iteration += 1\n",
        "\n",
        "print(f\"Policy Iteration converged in {iteration} outer iterations.\")\n",
        "print(\"Final Value Function:\")\n",
        "print(V.reshape(grid_size, grid_size))\n",
        "\n",
        "print(\"\\nImproved Policy:\")\n",
        "for s in range(states):\n",
        "    if s in [0, 15]:\n",
        "        print(f\"State {s}: Terminal\")\n",
        "    else:\n",
        "        best_action = max(policy[s], key=policy[s].get)\n",
        "        print(f\"State {s}: {best_action}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_HG7ZFCfhYGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0a6f23-6b2c-42a1-bf63-ee71f529eae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Value Function\n",
            "[[  0.         -13.99893866 -19.99842728 -21.99824003]\n",
            " [-13.99893866 -17.99861452 -19.9984378  -19.99842728]\n",
            " [-19.99842728 -19.9984378  -17.99861452 -13.99893866]\n",
            " [-21.99824003 -19.99842728 -13.99893866   0.        ]]\n",
            "\n",
            "Current Policy:\n",
            "State 0: Terminal\n",
            "State 1: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 2: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 3: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 4: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 5: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 6: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 7: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 8: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 9: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 10: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 11: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 12: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 13: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 14: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 15: Terminal\n",
            "____________________________________________________________\n",
            "\n",
            "Iteration 1: Value Function\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Current Policy:\n",
            "State 0: Terminal\n",
            "State 1: {'up': 0.0, 'down': 0.0, 'left': 1.0, 'right': 0.0}\n",
            "State 2: {'up': 0.0, 'down': 0.0, 'left': 1.0, 'right': 0.0}\n",
            "State 3: {'up': 0.0, 'down': 0.5, 'left': 0.5, 'right': 0.0}\n",
            "State 4: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}\n",
            "State 5: {'up': 0.0, 'down': 0.0, 'left': 1.0, 'right': 0.0}\n",
            "State 6: {'up': 0.0, 'down': 0.5, 'left': 0.5, 'right': 0.0}\n",
            "State 7: {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0}\n",
            "State 8: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}\n",
            "State 9: {'up': 0.5, 'down': 0.0, 'left': 0.0, 'right': 0.5}\n",
            "State 10: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0}\n",
            "State 11: {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0}\n",
            "State 12: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}\n",
            "State 13: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0}\n",
            "State 14: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0}\n",
            "State 15: Terminal\n",
            "____________________________________________________________\n",
            "\n",
            "Iteration 2: Value Function\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Current Policy:\n",
            "State 0: Terminal\n",
            "State 1: {'up': 0.0, 'down': 0.0, 'left': 1.0, 'right': 0.0}\n",
            "State 2: {'up': 0.0, 'down': 0.0, 'left': 1.0, 'right': 0.0}\n",
            "State 3: {'up': 0.0, 'down': 0.5, 'left': 0.5, 'right': 0.0}\n",
            "State 4: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}\n",
            "State 5: {'up': 0.5, 'down': 0.0, 'left': 0.5, 'right': 0.0}\n",
            "State 6: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 7: {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0}\n",
            "State 8: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}\n",
            "State 9: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
            "State 10: {'up': 0.0, 'down': 0.5, 'left': 0.0, 'right': 0.5}\n",
            "State 11: {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0}\n",
            "State 12: {'up': 0.5, 'down': 0.0, 'left': 0.0, 'right': 0.5}\n",
            "State 13: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0}\n",
            "State 14: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0}\n",
            "State 15: Terminal\n",
            "____________________________________________________________\n",
            "\n",
            "Policy Iteration converged in 3 outer iterations.\n",
            "Final Value Function:\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Improved Policy:\n",
            "State 0: Terminal\n",
            "State 1: left\n",
            "State 2: left\n",
            "State 3: down\n",
            "State 4: up\n",
            "State 5: up\n",
            "State 6: up\n",
            "State 7: down\n",
            "State 8: up\n",
            "State 9: up\n",
            "State 10: down\n",
            "State 11: down\n",
            "State 12: up\n",
            "State 13: right\n",
            "State 14: right\n",
            "State 15: Terminal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import poisson\n",
        "\n",
        "MAX_CARS = 20\n",
        "MAX_MOVE = 5\n",
        "RENTAL_REWARD = 10\n",
        "MOVE_COST = 2\n",
        "GAMMA = 0.9\n",
        "THRESHOLD = 1e-2\n",
        "\n",
        "# Poisson parameters\n",
        "RENTAL_REQUESTS = [3, 4]\n",
        "RETURNS = [3, 2]\n",
        "\n",
        "# Precompute Poisson probabilities up to 11\n",
        "poisson_cache = dict()\n",
        "\n",
        "def poisson_prob(n, lam):\n",
        "    key = (n, lam)\n",
        "    if key not in poisson_cache:\n",
        "        poisson_cache[key] = poisson.pmf(n, lam)\n",
        "    return poisson_cache[key]\n",
        "\n",
        "# State: (cars at loc1, cars at loc2)\n",
        "states = [(i, j) for i in range(MAX_CARS + 1) for j in range(MAX_CARS + 1)]\n",
        "V = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
        "policy = np.zeros((MAX_CARS + 1, MAX_CARS + 1), dtype=int)\n",
        "\n",
        "# Expected return function\n",
        "def expected_return(state, action, V):\n",
        "    cars1, cars2 = state\n",
        "    # Apply action\n",
        "    action = int(action)\n",
        "    cars1_ = min(cars1 - action, MAX_CARS)\n",
        "    cars2_ = min(cars2 + action, MAX_CARS)\n",
        "    if cars1_ < 0 or cars2_ < 0:\n",
        "        return -np.inf  # Invalid action\n",
        "\n",
        "    reward = -MOVE_COST * abs(action)\n",
        "    expected = 0.0\n",
        "\n",
        "    for rent1 in range(0, 11):\n",
        "        prob_rent1 = poisson_prob(rent1, RENTAL_REQUESTS[0])\n",
        "        real_rent1 = min(cars1_, rent1)\n",
        "        ret_reward1 = real_rent1 * RENTAL_REWARD\n",
        "        cars1_after_rent = cars1_ - real_rent1\n",
        "\n",
        "        for rent2 in range(0, 11):\n",
        "            prob_rent2 = poisson_prob(rent2, RENTAL_REQUESTS[1])\n",
        "            real_rent2 = min(cars2_, rent2)\n",
        "            ret_reward2 = real_rent2 * RENTAL_REWARD\n",
        "            cars2_after_rent = cars2_ - real_rent2\n",
        "\n",
        "            for ret1 in range(0, 11):\n",
        "                prob_ret1 = poisson_prob(ret1, RETURNS[0])\n",
        "                cars1_final = min(cars1_after_rent + ret1, MAX_CARS)\n",
        "\n",
        "                for ret2 in range(0, 11):\n",
        "                    prob_ret2 = poisson_prob(ret2, RETURNS[1])\n",
        "                    cars2_final = min(cars2_after_rent + ret2, MAX_CARS)\n",
        "\n",
        "                    prob = prob_rent1 * prob_rent2 * prob_ret1 * prob_ret2\n",
        "                    total_reward = ret_reward1 + ret_reward2 + reward\n",
        "                    expected += prob * (total_reward + GAMMA * V[cars1_final, cars2_final])\n",
        "    return expected\n",
        "\n",
        "# Policy Iteration\n",
        "iteration = 0\n",
        "while True:\n",
        "    # Policy Evaluation\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(MAX_CARS + 1):\n",
        "            for j in range(MAX_CARS + 1):\n",
        "                v = V[i, j]\n",
        "                action = policy[i, j]\n",
        "                V[i, j] = expected_return((i, j), action, V)\n",
        "                delta = max(delta, abs(v - V[i, j]))\n",
        "        if delta < THRESHOLD:\n",
        "            break\n",
        "\n",
        "    # Policy Improvement\n",
        "    policy_stable = True\n",
        "    for i in range(MAX_CARS + 1):\n",
        "        for j in range(MAX_CARS + 1):\n",
        "            old_action = policy[i, j]\n",
        "            action_returns = []\n",
        "            for a in range(-MAX_MOVE, MAX_MOVE + 1):\n",
        "                if 0 <= i - a <= MAX_CARS and 0 <= j + a <= MAX_CARS:\n",
        "                    val = expected_return((i, j), a, V)\n",
        "                    action_returns.append((val, a))\n",
        "            _, best_action = max(action_returns)\n",
        "            policy[i, j] = best_action\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "\n",
        "    iteration += 1\n",
        "    print(f\"Iteration {iteration} complete.\")\n",
        "    if policy_stable:\n",
        "        print(\"Policy stable — converged.\")\n",
        "        break\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"\\nFinal Policy (action = cars moved from loc1 to loc2):\")\n",
        "print(policy)\n",
        "print(\"\\nFinal Value Function:\")\n",
        "print(np.round(V, 1))\n"
      ],
      "metadata": {
        "id": "NjyaXBufIAHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CyImHVFUGb2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}