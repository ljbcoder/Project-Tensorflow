{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7+QsCahOoEk/wQAaDIT6O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljbcoder/Project-Tensorflow/blob/main/Project_Tensorflow_AI_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What's Going on: Directly Under the Hood\n",
        "import numpy as np\n",
        "\n",
        "I = np.array([[0.05,.10]])\n",
        "T = np.array([[0,1]])\n",
        "WH = np.array([[0.15, 0.25],\n",
        "               [0.2,0.3]])\n",
        "\n",
        "BH = np.array([[0.35,0.35]])\n",
        "WO = np.array([[0.4, 0.5],\n",
        "               [0.45,0.55]])\n",
        "BO = np.array([[0.6,.6]])\n",
        "\n",
        "for epoch in range(100000):\n",
        "  print(epoch)\n",
        "\n",
        "  #Apply sigmoid to first layer\n",
        "  H = I @ WH + BH\n",
        "  H = 1/(1+np.exp(-H))\n",
        "\n",
        "  #Apply Softmax for second layer\n",
        "  O = H @ WO + BO\n",
        "  OM = O - np.max(O)\n",
        "  O = np.exp(OM)/np.sum(np.exp(OM))\n",
        "\n",
        "  E = np.sum(-T*np.log(O))\n",
        "  if E < 0.0001:\n",
        "    break\n",
        "\n",
        "  Ob = O - T\n",
        "  #Nothing for softmax cross entropy error\n",
        "\n",
        "  Hb = Ob @ WO.T\n",
        "  Hb = Hb * H * (1-H)\n",
        "\n",
        "  WHb = I.T @ Hb\n",
        "  BHb = 1* Hb\n",
        "  WOb = H.T @ Ob\n",
        "  BOb = 1 * Ob\n",
        "\n",
        "  lr = 0.01\n",
        "  WH = WH - lr * WHb\n",
        "  BH = BH - lr * BHb\n",
        "  WO = WO - lr * WOb\n",
        "  BO = BO - lr - BOb\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mi5kWcIWqHHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINAL CODE**"
      ],
      "metadata": {
        "id": "8f-QwhcYCjQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "ww4fmNlykgMs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train.reshape(-1, 28*28).T / 255.0  # shape: (784, num_samples)\n",
        "x_test = x_test.reshape(-1, 28*28).T / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train).T  # shape: (10, num_samples)\n",
        "y_test = to_categorical(y_test).T\n"
      ],
      "metadata": {
        "id": "p9ctQJqIkoFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5980e0-8107-4afa-c6e5-af82cf1a2427"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Each image corresponds to a column of x --> 28x28 pixels / 60000 images\n",
        "#Each Label corresponds to a column of y --> 10 categories / 60000 labels\n",
        "print(\"Shape of x_train: \",x_train.shape)\n",
        "print(\"Shape of y_train: \",y_train.shape)\n",
        "print(\"Shape of x_test: \",x_test.shape)\n",
        "print(\"Shape of y_test: \",y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKEiSHvUezuU",
        "outputId": "c588073c-9bbb-4747-f503-2c2ad22e3646"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_train:  (784, 60000)\n",
            "Shape of y_train:  (10, 60000)\n",
            "Shape of x_test:  (784, 10000)\n",
            "Shape of y_test:  (10, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "#I put Softmax derivative as implicit: Cancels out with Cross entropy\n",
        "# def softmax_derivative(z):\n",
        "#     s = softmax(z)\n",
        "#     return s * (1 - s) --> This eventually cancels out with 1/s multiplied in the backpropagation step\n",
        "\n",
        "#Loss Functions\n",
        "def MSE(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def MSE_derivative(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true)\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    epsilon = 1e-10  # Small value to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Clip values to avoid log(0)\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "\n",
        "def cross_entropy_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true\n",
        "\n",
        "class ModelLayer:\n",
        "    def __init__(self, output_size, activation='relu', input_size=None, learning_rate=0.001):\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size  # Set later if None\n",
        "        self.activation_name = activation.lower()\n",
        "        self.activation = None\n",
        "        self.activation_derivative = None\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights = None  # Will be initialized when input_size is known\n",
        "        self.bias = None\n",
        "\n",
        "        self.input_vals = None\n",
        "        self.z = None\n",
        "\n",
        "    def build(self, input_size):\n",
        "        \"\"\"Initialize weights and biases once input size is known\"\"\"\n",
        "        self.input_size = input_size\n",
        "\n",
        "        if self.activation_name == 'relu':\n",
        "            # He initialization (Used mostly for relu, leaky relu) --> Weight initializations to avoid vanishing/exploding gradient\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = relu\n",
        "            self.activation_derivative = relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'leaky_relu':\n",
        "            # He initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            self.activation = leaky_relu\n",
        "            self.activation_derivative = leaky_relu_derivative\n",
        "\n",
        "        elif self.activation_name == 'sigmoid':\n",
        "            # Xavier initialization -> (Used mostly for Sigmoid, tanh, softmax) --> Weight initializations to avoid vanishing/exploding gradient\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(1. / input_size)\n",
        "            self.activation = sigmoid\n",
        "            self.activation_derivative = sigmoid_derivative\n",
        "\n",
        "        elif self.activation_name == 'softmax':\n",
        "            # Xavier initialization\n",
        "            self.weights = np.random.randn(self.output_size, input_size) * np.sqrt(1. / input_size)\n",
        "            self.activation = softmax\n",
        "            self.activation_derivative = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {self.activation_name}\")\n",
        "\n",
        "        self.bias = np.zeros((self.output_size, 1))\n",
        "\n",
        "    #Forward Propagation\n",
        "    def pass_forward(self, input_vals):\n",
        "        if self.weights is None:\n",
        "            self.build(input_vals.shape[0])\n",
        "        self.input_vals = input_vals\n",
        "        self.z = np.dot(self.weights, input_vals) + self.bias\n",
        "        return self.activation(self.z)\n",
        "\n",
        "    #Backpropagation\n",
        "    def pass_backwards(self, dA):\n",
        "        if self.activation_name == 'softmax':\n",
        "            dZ = dA  # Softmax derivative is handled implicitly in cross-entropy\n",
        "        else:\n",
        "          dZ = dA * self.activation_derivative(self.z)\n",
        "\n",
        "        dW = np.dot(dZ, self.input_vals.T)\n",
        "        dB = np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = np.dot(self.weights.T, dZ)\n",
        "\n",
        "        self.weights -= self.learning_rate * dW\n",
        "        self.bias -= self.learning_rate * dB\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "    def reset_weights(self):\n",
        "        self.build(self.input_size)\n",
        "\n",
        "class Model:\n",
        "    def __init__(self,layers = []):\n",
        "        self.layers = layers\n",
        "        self.built = False\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    #Builds layers after all layer_sizes have been set\n",
        "    def _build_layers(self, input_size):\n",
        "        for layer in self.layers:\n",
        "            if layer.input_size is None:\n",
        "                layer.build(input_size)\n",
        "            input_size = layer.output_size  # next layer's input\n",
        "\n",
        "    #Set optimizers and loss functions\n",
        "    def compile(self, optimizer, loss_function):\n",
        "        self.optimizer = optimizer.lower()\n",
        "\n",
        "        self.loss_function_name = loss_function.lower()\n",
        "\n",
        "        #Apply the loss function\n",
        "        if self.loss_function_name == 'mse':\n",
        "          self.loss_function = MSE\n",
        "          self.loss_function_derivative = MSE_derivative\n",
        "\n",
        "        elif self.loss_function_name == 'cross_entropy':\n",
        "          self.loss_function = cross_entropy\n",
        "          self.loss_function_derivative = cross_entropy_derivative\n",
        "\n",
        "        else:\n",
        "          raise ValueError(f\"Unsupported Loss Function: {loss_function}\")\n",
        "\n",
        "    #Forward Propagation\n",
        "    def forward(self, x):\n",
        "        if not self.built:\n",
        "            self._build_layers(x.shape[0])\n",
        "            self.built = True\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.pass_forward(x)\n",
        "        return x\n",
        "\n",
        "    #Backpropagation\n",
        "    def backward(self, dLoss):\n",
        "        for layer in reversed(self.layers):\n",
        "            dLoss = layer.pass_backwards(dLoss)\n",
        "\n",
        "    #Train the model!\n",
        "    def train(self, x, y_true, epochs=20, batch_size = 100, print_every=1):\n",
        "        if self.optimizer == 'sgd':\n",
        "            for epoch in range(epochs):\n",
        "                for start in range(0, len(x[0]), batch_size):  # Process in batches of 100 samples\n",
        "                    x_batch = x[:, start:start+batch_size]  # Shape: (784, 100)\n",
        "                    y_batch = y_true[:, start:start+batch_size]  # Shape: (10, 100)\n",
        "\n",
        "                    # Forward pass\n",
        "                    y_pred = self.forward(x_batch)  # Shape: (10, 100)\n",
        "\n",
        "                    # Compute the loss for the entire batch\n",
        "                    loss = self.loss_function(y_batch, y_pred)  # Shape: (10, 100)\n",
        "                    dLoss = self.loss_function_derivative(y_batch, y_pred)\n",
        "\n",
        "                    # Backward pass\n",
        "                    self.backward(dLoss)\n",
        "\n",
        "                if epoch % print_every == 0 or epoch == epochs - 1:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer: {self.optimizer}\")\n",
        "\n",
        "    def reset_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.reset_weights()\n",
        "\n",
        "    #Predictions are just forward propagations of a given input\n",
        "    def predict(self, x):\n",
        "      return self.forward(x)\n",
        "\n",
        "    #Evaluate the model on test data\n",
        "    def evaluate(self, x_test, y_test):\n",
        "      predictions = self.predict(x_test)  # Predict for inputs all at once (batch prediction)\n",
        "      predicted_classes = np.argmax(predictions, axis=0)  # axis=0 --> shape is (num_classes, num_samples)\n",
        "      true_classes = np.argmax(y_test, axis=0)\n",
        "\n",
        "      accuracy = np.mean(predicted_classes == true_classes)\n",
        "      return float(accuracy)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SpQZRAjiMKlf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def evaluate(self,x_test, y_test):\n",
        "        correct = 0\n",
        "        for i in range(len(x_test[0])):\n",
        "            current_image = x_test[:, i, None]\n",
        "            prediction = self.predict(current_image)\n",
        "            label = y_test[:,i]\n",
        "            if np.argmax(prediction) == label:\n",
        "                correct += 1\n",
        "        return correct / len(x_test[0]) #Finding the average score of the values"
      ],
      "metadata": {
        "id": "4Hn2mWFBaOe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "model.add(ModelLayer(128, activation='relu'))\n",
        "model.add(ModelLayer(64, activation='relu'))\n",
        "model.add(ModelLayer(32, activation='relu'))\n",
        "model.add(ModelLayer(32, activation='relu'))\n",
        "model.add(ModelLayer(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=\"sgd\", loss_function=\"cross_entropy\")\n",
        "model.train(x_train[:, :2000], y_train[:, :2000], epochs=50, print_every=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzGHlhhImegv",
        "outputId": "c93e3cd8-1bcb-4363-ce6a-629c26e1298b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 166.0571\n",
            "Epoch 10, Loss: 10.5235\n",
            "Epoch 20, Loss: 1.6900\n",
            "Epoch 30, Loss: 0.6477\n",
            "Epoch 40, Loss: 0.3210\n",
            "Epoch 49, Loss: 0.2123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def test_prediction(index):\n",
        "    current_image = x_test[:, index, None]\n",
        "    prediction = model.predict(current_image)\n",
        "    label = y_test[:,index]\n",
        "\n",
        "\n",
        "    print(\"Model Output: \\n\", prediction)\n",
        "    print(\"Prediction: \", np.argmax(prediction))\n",
        "    print(\"Label: \", np.argmax(label))\n",
        "\n",
        "    current_image = current_image.reshape((28, 28)) * 255\n",
        "    plt.gray()\n",
        "    plt.imshow(current_image, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "test_prediction(np.random.randint(0,10000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "fWiUOywl-xNr",
        "outputId": "4ebfe18a-5977-4deb-dd81-cb2ed2165747"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output: \n",
            " [[8.78315240e-06]\n",
            " [4.20717488e-08]\n",
            " [9.99799406e-01]\n",
            " [1.67140985e-04]\n",
            " [2.24411566e-09]\n",
            " [3.32277144e-10]\n",
            " [1.62514962e-06]\n",
            " [5.41996310e-11]\n",
            " [2.27947403e-05]\n",
            " [2.05508942e-07]]\n",
            "Prediction:  2\n",
            "Label:  2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y92cpGbQTlV6",
        "outputId": "ec716bd2-0e72-4553-fcf9-f11deaba4faa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9047"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDmEjP5lgJew",
        "outputId": "2c245ae1-30f0-4e0d-b790-79b89877192c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}